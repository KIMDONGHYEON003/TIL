# 1003_STUDY

- 과대적합 : 훈련 데이터에서 성능이 좋지만 교차 검증 점수가 나쁘면
- 과소 적합 : 훈련 데이터의 성능도 안좋고 교차 검증 점수도 나쁜 경우
- 편향
  - 잘못된 가정으로 인한 것
  - 편향이 큰 모델은 훈련 데이터에 과소적합되기 쉽다
- 분산 
  - 훈련 데이터에 있는 작은 변동에 모델이 과도하게 민감하기 때문에 나타남
  - 자유도가 높은 모델은 높은 분산을 가지기 쉬워 훈련 데이터에 과대적합되는 경향
- 줄일 수 없는 오차
  - 데이터 자체에 있는 잡음 때문에 발생
  - 잡은 제거
- 모델 복잡도 증가 / 분산 증가 / 편향 감소
- 모델 복잡도 감소 / 분산 감소 / 편향 증가



- ### 규제가 있는 선형 모델

  - 과대 적합을 감소기키는 좋은 방법

  - 다항식의 차수를 감소시키다

  - ##### 릿지 회귀

    - 규제항을 사용 : 모델의 가중치가 가능한 작게 유지되도록 노력
    - 훈련이 끝나면 모델의 성능을 규제 없는 성능 지표로 평가
    - 하이퍼 파라미터로 얼마나 규제할 지 조절
    - 하이퍼 파라미터를 증가시킬 수록 직선에 가까워진다 : 모델의 분산은 줄고 편향은 커진다

  - ##### 라쏘 회귀

    - 덜 중요한 특성의 가중치를 제거하려고 함
    - 자동으로 특성 선택을 하고 희소 모델을 만든다
    - 라쏘와 릿지의 차아
      - 파라미터가 전역 최적점에 가까워질수록 그레디언트가 작아짐 -> 경사 하강법이 자동으로 느려지고 수렴에  도움이 됨
      - 하이퍼 파라미터를 증가시킬수록 최적의 파라미터가 원점에 더 가까워짐
    - 라쏘를 사용할 때 경사하강법이 최적점 근처에서 진동하는 것을 막으려면 훈련하는 동안 점진적으로 학습률을 감소시켜야함
    - 서브그레디언트 벡터 : 경사하강법을 적용할 때

  - 엘라스틱넷

    - 릿지와 라쏘를 절충한 모델
    - 규제항은 릿지와 회귀의 규제항을 더해서 사용
    - r로 혼합 비율을 조절
    - r=0 릿지 / r=1 라쏘

  - 조기종료

    - 검증에러가 최솟값에 도달하면 바로 훈련을 중지

      

- ### 로지스틱 회귀

  - 샘플이 특정 클래스에 속할 활률을 추정하는데 널리 사용
  - 추정 확률이 50%가 넘으면 클래스에 속한다고 예측 -> 이진분류기
  - 확률 추정 ( **어떻게 작동?**)
    - 입력 특성의 가중치 합을 계산한다 그리고 편향을 더한다
    - 결과 값의 로지스틱을 출력
    - 로지스틱 : 0과 1 사이의 값을 출력하는 시그모이드 함수
  - 훈련과 비용 함수 ( **어떻게 훈련 시킬까?**)
    - 목적 : 양성 샘플에 대해서는 높은 확률을 추정, 음성 샘플에 대해서는 낮은 확류을 추정하는 모델 파라미터 벡터를 찾는다.
    - 양성 샘플을 0에 가까운 확률로 추정하면 비용이 크게 증가
    - 음성 샘플을 1에 가까운 확률로 추정하면 비용이 증가
    - 음성 샘플 확률을 0에 가깝게 추정하거나 양성 샘플의 확률을 1에 가깝게 추정하면 비용은 0에 가까워짐
  - 로그 손실
    - 모든 훈련 샘플의 비용을 평균한 것
    - 볼록함수이므로 경사하강법이 최솟값을 찾
  - 결정 경계
    - 기본 벡터공간을 각 클래스에 대하여 하나씩 두 개의 집합으로 나누는 초표면이다. 
    - 분류기는 결정 경계의 한쪽에 있는 모든 점을 한 클래스, 다른 한쪽에 있는 모든 점을 다른 클래스에 속하는 것으로 분류
  - 소프트맥스 회귀
    - 모델의 아웃풋에 softmax 함수를 적용해서 모델의 출력값이 각각의 레이블에 대한 확신의 정도를 출력
    - 여러 개의 이진 분류기를 훈련시켜 연결하지 않고 직접 다중 클래스를 지원하도록 일반화
    - 다항 로지스틱 회귀
    - 소프트 맥스 함수(정규화된 지수 함수)
    - 다중 클래스는 맞지만 다중 출력은 아니다(하나의 사진에서 여러 사람의 얼굴을 인식하는데는 사용할 수 없다)
  - 크로스 엔트로피 (시그모이드)
    - 모델이 타깃 클래스에 대해 높은 확률을 추정하도록 하는 것이 목적이다
    - 크로스 엔트로피 비용함수를 최소화하는 것 : 타깃 클래스에 대해 낮은 확률을 예측하는 모델을 억제
    - 추정된 클래스의 확률이 타깃 클래스에 얼마나 잘 맞는지 측정하는 용도로 사용
    - 모델의 예측값이 참 값과 비슷하면 작은 값, 참값과 다르면 큰 값을 갖는다
    - 실제 값과 예측값이 맞는 경우 0에 수렴/ 틀릴 경우 값이 커진다
    - 실제값과 예측값의 차이를 줄이기 위한 엔트로피







- 3. 로지스틱 회귀 모델의 비용함수는 볼록함수이므로 경사하강법에 갇히지 않는다
- 8. 검증 오차가 훈련 오차보다 훨씬 높으면 모델이 훈련세트에 과대적합된것 -> 차수를 낮춘다 (자유도를 줄이면 과대적합이 줄어들 것) / 모델 규제 라쏘 릿지 / 훈련 세트 크기 증가
- 9. 훈련에러 검증에러가 거의 비슷하고 매우 높다면 과소적합 -> 높은 편향을 가진 모델 / 규제/ 하이퍼 파라미터 감소
- 10. 1. 규제가 있는 선형 모델이 없는 모델보다 성능이 좋다.
      2. 자동으로 특성 선택을 하기때문에 유용 
  11.  두개의 로지스틱을 사용한다. / 